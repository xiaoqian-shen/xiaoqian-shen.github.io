<!DOCTYPE html>
<html lang="en-US">
  <head>
    <title>Xiaoqian Shen</title>

    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="The Minimal Light is a simple and elegant jekyll theme for academic personal homepage.">
    
    <meta name="keywords" content="minimal light">
    
    
    <link rel="canonical" href="https://minimal-light-theme.yliu.me/"/>
    

    <link rel="icon" media="(prefers-color-scheme:dark)" href="" type="image/png" />
    <link rel="icon" media="(prefers-color-scheme:light)" href="" type="image/png" />
    <script src="./assets/js/favicon-switcher.js" type="application/javascript"></script>

    <link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin=anonymous>
    <link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin=anonymous>

    
    <link rel="stylesheet" href="./assets/css/font.css">
    
    <link rel="stylesheet" href="./assets/css/style.css">
    <link rel="stylesheet" href="./assets/css/publications.css">

  </head>
  <body>
    <div class="wrapper">
      <header>
        
        
        <a class="image avatar"><img src="./assets/img/profile.jpg" alt="avatar" /></a>
        

        <h1>Xiaoqian Shen</h1>

        
        <position style="font-size:1.10rem;">KAUST CS PhD</position>
        <br>
        
        
        
        <email>xiaoqian.shen (at) kaust.edu.sa</email>
        

        <br>
        <br>
        <div class="social-icons">
        
        <a style="margin: 0 5px 0 0" target="_blank" href="https://scholar.google.com/citations?hl=en&user=uToGtIwAAAAJ">
          <i class="ai ai-google-scholar" style="font-size:1.2rem"></i>
        </a>  
        

        
        <a style="margin: 0 5px 0 0" target="_blank" href="assets/files/resume.pdf">
          <i class="ai ai-cv" style="font-size:1.3rem;"></i>
        </a>
        

        
        <a style="margin: 0 5px 0 0" target="_blank" href="https://github.com/xiaoqian-shen">
          <i class="fab fa-github"></i>
        </a>
        

        
        <a style="margin: 0 5px 0 0" target="_blank" href="https://www.linkedin.com/in/xiaoqian-shen-759991264">
          <i class="fab fa-linkedin"></i>
        </a>
        

        
        <a style="margin: 0 0 0 0" target="_blank" href="https://twitter.com/xiaoqian_shen">
          <i class="fab fa-twitter"></i>
        </a>
        
        </div>
        <br>

      </header>
      <section>

      <h2 id="about-me">About Me</h2>

<p>I am currently a PhD student of Computer Science at <a href="https://cemse.kaust.edu.sa/">King Abdullah University of Science and Technology</a> supervised by <a href="https://cemse.kaust.edu.sa/people/person/mohamed-elhoseiny">Mohamed Elhoseiny</a>. Before that, I received BSc in Computer Science from <a href="https://www.jlu.edu.cn/">Jilin University</a>, China. I have been fortunate to develop research and industry experience through internships at Meta and Nvidia.</p>

<h2 id="research-interests">Research Interests</h2>

<style>
  .paper-tag {
    font-size: 0.7em;
    color: #555;
    font-weight: normal;
  }

  .paper-link {
    text-decoration: none;
    color: #069;
    font-weight: normal;
  }
</style>

<ul>
  <li><strong>Generative Models:</strong> Image Generation, Video Generation (<a class="paper-link" href="https://arxiv.org/abs/2304.02777">MoStGAN-V<sub class="paper-tag">CVPR</sub></a>, <a class="paper-link" href="https://arxiv.org/abs/2312.02252">StoryGPT-V<sub class="paper-tag">CVPR</sub></a>)</li>
  <li><strong>Vision-Language:</strong> Multi-Modal Comprehension (<a class="paper-link" href="">Vgent<sub class="paper-tag">NeurIPS</sub></a>, <a class="paper-link" href="https://arxiv.org/abs/2410.17434">LongVU<sub class="paper-tag">ICML</sub></a>, <a class="paper-link" href="https://arxiv.org/abs/2304.10592">MiniGPT-4<sub class="paper-tag">ICLR</sub></a>)</li>
</ul>

<h2 id="news">News</h2>

<style>
  #scrollableDiv {
    min-height: 100px;
    height: 100px;
    overflow-y: hidden;
    opacity: 1;
    transition: height 0.5s ease-in-out, opacity 0.5s ease-in-out;
  }
</style>

<ul id="scrollableDiv" onmouseover="showScrollbar()" onmouseout="hideScrollbar()">
  <li><strong>[Sep. 2025]</strong> One paper (<a href="">Vgent</a>) gets accepted to NeurIPSâ€™25 (Spotlight) ðŸŽ‰</li>
  <li><strong>[May. 2025]</strong> One paper (<a href="https://arxiv.org/abs/2410.17434">LongVU</a>) gets accepted to ICMLâ€™25 ðŸŽ‰</li>
  <li><strong>[Feb. 2025]</strong> One paper (<a href="https://arxiv.org/abs/2312.02252">StoryGPT-V</a>) gets accepted to CVPRâ€™25 ðŸŽ‰</li>
  <li><strong>[July. 2024]</strong> Two papers (<a href="https://arxiv.org/abs/2407.12679">GoldFish</a>, <a href="https://arxiv.org/abs/2308.16349">AffectVisDial</a>) get accepted to ECCVâ€™24</li>
  <li><strong>[Mar. 2024]</strong> One paper (<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Haydarov_Adversarial_Text_to_Continuous_Image_Generation_CVPR_2024_paper.pdf">HyperCGAN</a>) gets accepted to CVPRâ€™24</li>
  <li><strong>[Jan. 2024]</strong> One paper (<a href="https://arxiv.org/abs/2304.10592">MiniGPT-4</a>) gets accepted to ICLRâ€™24</li>
  <li><strong>[Nov. 2023]</strong> Successfully defended my Master thesis</li>
  <li><strong>[July. 2023]</strong> One paper (<a href="https://arxiv.org/abs/2304.05390">HRS-Bench</a>) gets accepted to ICCVâ€™23</li>
  <li><strong>[Feb. 2023]</strong> One paper (<a href="https://arxiv.org/abs/2304.02777">MoStGAN-V</a>) gets accepted to CVPRâ€™23</li>
  <li><strong>[Sep. 2022]</strong> Started my Master journey at KAUST</li>
  <li><strong>[July. 2022]</strong> One paper (<a href="https://arxiv.org/abs/2203.01386">HGR-Net</a>) gets accepted to ECCVâ€™22</li>
  <li><strong>[Dec. 2021]</strong> Joined Vision-CAIR at KAUST as a visiting research student</li>
</ul>

<p></p>
<script>
  function showScrollbar() {
    var div = document.getElementById('scrollableDiv');
    div.style.height = div.scrollHeight + 'px';
    div.style.opacity = 1;
  }
  function hideScrollbar() {
    var div = document.getElementById('scrollableDiv');
    div.style.height = '100px';
    div.style.opacity = 1;
  }
</script>

<h2 id="experience">Experience</h2>

<ul style="margin:0 0 5px;">
<li style="display: flex; justify-content: space-between; align-items: center; margin-left: -30px;">
  <span>
    <img src="../assets/img/org/nvidia.png" alt="Meta" width="20" height="20" style="vertical-align: middle; margin-bottom: 4px" />
     Research Intern, Taiwan, <strong>Nvidia</strong>
  </span>
  <span>Jun 2025 - Sep 2025</span>
</li>
<li style="display: flex; justify-content: space-between; align-items: center; margin-left: -30px; margin-bottom: 4px">
    <span>
        <img src="../assets/img/org/meta.png" alt="Meta" width="20" height="20" style="vertical-align: middle;" />
         Research Scientist Intern, XR Core AI, <strong>Meta</strong>
    </span>
    <span>May 2024 - Nov 2024</span>
</li> 
<li style="display: flex; justify-content: space-between; align-items: center; margin-left: -30px; margin-bottom: 7px">
    <span>
        <img src="../assets/img/org/kaust.png" alt="Meta" width="20" height="20" style="vertical-align: middle;" />
         Visiting Research Student at Mohamed Elhoseiny's Group, <strong>KAUST</strong>
    </span>
    <span>Dec 2021 - Mar 2022</span>
</li>
<li style="display: flex; justify-content: space-between; align-items: center; margin-left: -30px;">
    <span>
        <img src="../assets/img/org/tsinghua.png" alt="Meta" width="20" height="20" style="vertical-align: middle;" />
         Research Assistant at Yongfeng Huang's Group, <strong>Tsinghua University</strong>
    </span>
    <span>Dec 2020 - Mar 2021</span>
</li>
</ul>

<p><br /></p>
<h2 id="publications" style="margin: 2px 0px -15px;">Publications <temp style="font-size:15px;">[</temp><a href="https://scholar.google.com/citations?hl=en&amp;user=uToGtIwAAAAJ" target="_blank" style="font-size:15px;">Google Scholar</a><temp style="font-size:15px;">]</temp></h2>

<div class="publications">
<ol class="bibliography">
<!--
<div class="pub-type-filter">
  <div class="pub-type" data-type="all" onmouseover="showPublications('all')">All</div>
  <div class="pub-type" data-type="genai" onmouseover="showPublications('genai')">GenAI</div>
  <div class="pub-type" data-type="mllm" onmouseover="showPublications('mllm')">MLLM</div>
</div>
-->



<li class="pub-item" data-type="mllm">
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
     
    <img src="./assets/img/pub/zoom.png" playsinline="" class="teaser img-fluid z-depth-1" style="width=100;height=40%" />
    
    
    
     
    <abbrp class="badge">Preprint</abbrp>
    
  </div>
  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
      <div class="title"><a href="">Zoom-Zero: Reinforced Coarse-to-Fine Video Understanding via Temporal Zoom-in</a></div>
      <div class="author"><strong><u>Xiaoqian Shen</u></strong>, Min-Hung Chen, Yu-Chiang Frank Wang, Mohamed Elhoseiny, Ryo Hachiuma</div>
      <div class="periodical"><em>Arxiv preprint 2025</em>
      </div>
    <div class="links">
       
      <a href="https://xiaoqian-shen.github.io/Zoom-Zero/" target="_blank" class="btn btn-sm z-depth-0" role="button" style="font-size:12px;">Project Page</a>
      
      
       
      <a href="https://github.com/xiaoqian-shen/Zoom-Zero" target="_blank" class="btn btn-sm z-depth-0" role="button" style="font-size:12px;">Code</a>
      
      
       
      <strong> &nbsp; <i style="color:#e74d3c">Work done at Nvidia</i></strong>
      
      
    </div>
  </div>
</div>
<br />
</li>



<li class="pub-item" data-type="mllm">
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
     
    <img src="./assets/img/pub/vgent.png" playsinline="" class="teaser img-fluid z-depth-1" style="width=100;height=40%" />
    
    
     
    <abbr class="badge">NeurIPS</abbr>
    
    
  </div>
  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
      <div class="title"><a href="">Vgent: Graph-based Retrieval-Reasoning-Augmented Generation For Long Video Understanding</a></div>
      <div class="author"><strong><u>Xiaoqian Shen</u></strong>, Wenxuan Zhang, Jun Chen, Mohamed Elhoseiny</div>
      <div class="periodical"><em>NeurIPS 2025</em>
      </div>
    <div class="links">
       
      <a href="https://xiaoqian-shen.github.io/Vgent/" target="_blank" class="btn btn-sm z-depth-0" role="button" style="font-size:12px;">Project Page</a>
      
      
       
      <a href="https://github.com/xiaoqian-shen/Vgent" target="_blank" class="btn btn-sm z-depth-0" role="button" style="font-size:12px;">Code</a>
      
      
       
      <strong> &nbsp; <i style="color:#e74d3c">Spotlight (Top 3%)</i></strong>
      
      
    </div>
  </div>
</div>
<br />
</li>



<li class="pub-item" data-type="mllm">
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
     
    <img src="./assets/img/pub/longvu.png" playsinline="" class="teaser img-fluid z-depth-1" style="width=100;height=40%" />
    
    
     
    <abbr class="badge">ICML</abbr>
    
    
  </div>
  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
      <div class="title"><a href="https://arxiv.org/abs/2410.17434">LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding</a></div>
      <div class="author"><strong><u>Xiaoqian Shen</u></strong>, Yunyang Xiong, Changsheng Zhao, Lemeng Wu, Jun Chen, Chenchen Zhu, Zechun Liu, Fanyi Xiao, Balakrishnan Varadarajan, Florian Bordes, Zhuang Liu, Hu Xu, Hyunwoo J. Kim, Bilge Soran, Raghuraman Krishnamoorthi, Mohamed Elhoseiny, Vikas Chandra</div>
      <div class="periodical"><em>ICML 2025</em>
      </div>
    <div class="links">
       
      <a href="https://vision-cair.github.io/LongVU" target="_blank" class="btn btn-sm z-depth-0" role="button" style="font-size:12px;">Project Page</a>
      
       
      <a href="https://arxiv.org/abs/2410.17434" target="_blank" class="btn btn-sm z-depth-0" role="button" style="font-size:12px;">PDF</a>
      
       
      <a href="https://github.com/Vision-CAIR/LongVU" target="_blank" class="btn btn-sm z-depth-0" role="button" style="font-size:12px;">Code</a>
      
      
       
      <strong> &nbsp; <i style="color:#e74d3c">Work done at Meta</i></strong>
      
      
    </div>
  </div>
</div>
<br />
</li>



<li class="pub-item" data-type="mllm,genai">
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
    
    
    <video poster="" id="teaser" autoplay="" muted="" loop="" class="teaser img-fluid z-depth-1">
    <source src="./assets/img/pub/storygptv.mp4" type="video/mp4" />
    </video>
    
     
    <abbr class="badge">CVPR</abbr>
    
    
  </div>
  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
      <div class="title"><a href="https://arxiv.org/abs/2312.02252">StoryGPT-V: Large Language Models as Consistent Story Visualizers</a></div>
      <div class="author"><strong><u>Xiaoqian Shen</u></strong>, Mohamed Elhoseiny</div>
      <div class="periodical"><em>CVPR 2025</em>
      </div>
    <div class="links">
       
      <a href="https://xiaoqian-shen.github.io/StoryGPT-V" target="_blank" class="btn btn-sm z-depth-0" role="button" style="font-size:12px;">Project Page</a>
      
       
      <a href="https://arxiv.org/abs/2312.02252" target="_blank" class="btn btn-sm z-depth-0" role="button" style="font-size:12px;">PDF</a>
      
       
      <a href="https://github.com/xiaoqian-shen/StoryGPT-V" target="_blank" class="btn btn-sm z-depth-0" role="button" style="font-size:12px;">Code</a>
      
      
      
      
    </div>
  </div>
</div>
<br />
</li>



<li class="pub-item" data-type="mllm">
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
     
    <img src="./assets/img/pub/minigpt4.png" playsinline="" class="teaser img-fluid z-depth-1" style="width=100;height=40%" />
    
    
     
    <abbr class="badge">ICLR</abbr>
    
    
  </div>
  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
      <div class="title"><a href="https://arxiv.org/abs/2304.10592">MiniGPT-4: Enhancing Vision-language Understanding with Advanced Large Language Models</a></div>
      <div class="author">Deyao Zhu*, Jun Chen*, <strong><u>Xiaoqian Shen</u></strong>, Xiang Li, Mohamed Elhoseiny</div>
      <div class="periodical"><em>ICLR 2024</em>
      </div>
    <div class="links">
       
      <a href="https://minigpt-4.github.io" target="_blank" class="btn btn-sm z-depth-0" role="button" style="font-size:12px;">Project Page</a>
      
       
      <a href="https://arxiv.org/abs/2304.10592" target="_blank" class="btn btn-sm z-depth-0" role="button" style="font-size:12px;">PDF</a>
      
       
      <a href="https://github.com/Vision-CAIR/MiniGPT-4" target="_blank" class="btn btn-sm z-depth-0" role="button" style="font-size:12px;">Code</a>
      
      
       
      <strong> &nbsp; <i style="color:#e74d3c">Github 25k+ Stars, 3k+ Cites</i></strong>
      
      
    </div>
  </div>
</div>
<br />
</li>



<li class="pub-item" data-type="mllm">
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
     
    <img src="https://vision-cair.github.io/Goldfish_website/repo_imgs/teaser_fig_final_final.jpg" playsinline="" class="teaser img-fluid z-depth-1" style="width=100;height=40%" />
    
    
     
    <abbr class="badge">ECCV</abbr>
    
    
  </div>
  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
      <div class="title"><a href="https://arxiv.org/abs/2407.12679">Goldfish: Vision-Language Understanding of Arbitrarily Long Videos</a></div>
      <div class="author">Kirolos Ataallah, <strong><u>Xiaoqian Shen</u></strong>, Eslam Abdelrahman, Essam Sleiman, Mingchen Zhuge, Jian Ding, Deyao Zhu, JÃ¼rgen Schmidhuber, Mohamed Elhoseiny</div>
      <div class="periodical"><em>ECCV 2024</em>
      </div>
    <div class="links">
       
      <a href="https://vision-cair.github.io/Goldfish_website" target="_blank" class="btn btn-sm z-depth-0" role="button" style="font-size:12px;">Project Page</a>
      
       
      <a href="https://arxiv.org/abs/2407.12679" target="_blank" class="btn btn-sm z-depth-0" role="button" style="font-size:12px;">PDF</a>
      
       
      <a href="https://github.com/Vision-CAIR/MiniGPT4-video" target="_blank" class="btn btn-sm z-depth-0" role="button" style="font-size:12px;">Code</a>
      
      
      
      
    </div>
  </div>
</div>
<br />
</li>



<li class="pub-item" data-type="">
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
     
    <img src="./assets/img/pub/dial.png" playsinline="" class="teaser img-fluid z-depth-1" style="width=100;height=40%" />
    
    
     
    <abbr class="badge">ECCV</abbr>
    
    
  </div>
  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
      <div class="title"><a href="https://arxiv.org/abs/2308.16349">Affective Visual Dialog: A Large-Scale Benchmark for Emotional Reasoning Based on Visually Grounded Conversations</a></div>
      <div class="author">Kilichbek Haydarov, <strong><u>Xiaoqian Shen</u></strong>, Avinash Madasu, Mahmoud Salem, Li-Jia Li, Gamaleldin Elsayed, Mohamed Elhoseiny</div>
      <div class="periodical"><em>ECCV 2024</em>
      </div>
    <div class="links">
       
      <a href="https://affective-visual-dialog.github.io" target="_blank" class="btn btn-sm z-depth-0" role="button" style="font-size:12px;">Project Page</a>
      
       
      <a href="https://arxiv.org/abs/2308.16349" target="_blank" class="btn btn-sm z-depth-0" role="button" style="font-size:12px;">PDF</a>
      
       
      <a href="https://github.com/Vision-CAIR/affectiveVisDial" target="_blank" class="btn btn-sm z-depth-0" role="button" style="font-size:12px;">Code</a>
      
      
      
      
    </div>
  </div>
</div>
<br />
</li>



<li class="pub-item" data-type="mllm">
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
     
    <img src="./assets/img/pub/minigpt-v2.png" playsinline="" class="teaser img-fluid z-depth-1" style="width=100;height=40%" />
    
    
    
     
    <abbrp class="badge">Preprint</abbrp>
    
  </div>
  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
      <div class="title"><a href="https://arxiv.org/abs/2310.09478">MiniGPT-v2: Large Language Model as a Unified Interface for Vision-Language Multi-task Learning</a></div>
      <div class="author">Jun Chen, Deyao Zhu, <strong><u>Xiaoqian Shen</u></strong>, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, Mohamed Elhoseiny</div>
      <div class="periodical"><em>Arxiv preprint 2023</em>
      </div>
    <div class="links">
       
      <a href="https://minigpt-v2.github.io" target="_blank" class="btn btn-sm z-depth-0" role="button" style="font-size:12px;">Project Page</a>
      
       
      <a href="https://arxiv.org/abs/2310.09478" target="_blank" class="btn btn-sm z-depth-0" role="button" style="font-size:12px;">PDF</a>
      
       
      <a href="https://github.com/Vision-CAIR/MiniGPT-4" target="_blank" class="btn btn-sm z-depth-0" role="button" style="font-size:12px;">Code</a>
      
      
      
      
    </div>
  </div>
</div>
<br />
</li>



<li class="pub-item" data-type="genai">
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
     
    <img src="./assets/img/pub/mostgan.png" playsinline="" class="teaser img-fluid z-depth-1" style="width=100;height=40%" />
    
    
     
    <abbr class="badge">CVPR</abbr>
    
    
  </div>
  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
      <div class="title"><a href="https://arxiv.org/abs/2304.02777">MoStGAN-V: Video Generation with Temporal Motion Styles</a></div>
      <div class="author"><strong><u>Xiaoqian Shen</u></strong>, Xiang Li, Mohamed Elhoseiny</div>
      <div class="periodical"><em>CVPR 2023</em>
      </div>
    <div class="links">
       
      <a href="https://xiaoqian-shen.github.io/MoStGAN-V" target="_blank" class="btn btn-sm z-depth-0" role="button" style="font-size:12px;">Project Page</a>
      
       
      <a href="https://arxiv.org/abs/2304.02777" target="_blank" class="btn btn-sm z-depth-0" role="button" style="font-size:12px;">PDF</a>
      
       
      <a href="https://github.com/xiaoqian-shen/MoStGAN-V" target="_blank" class="btn btn-sm z-depth-0" role="button" style="font-size:12px;">Code</a>
      
      
      
      
    </div>
  </div>
</div>
<br />
</li>



</ol>
</div>

<h2 id="services">Services</h2>

<h4 style="margin:0 10px 0;">Conference Reviewers</h4>

<ul style="margin:0 0 5px;">
  <li>CVPR, ECCV, AAAI, ICLR, ICCV, AAAI</li>
  <li>SIGGRAPH Asia, NeurIPSW</li>
</ul>

<h4 style="margin:0 10px 0;">Journal Reviewers</h4>

<ul style="margin:0 0 20px;">
  <li>IJCV, CVIU</li>
</ul>

<h4 style="margin:0 10px 0;">Teaching Assistant</h4>

<ul style="margin:0 0 20px;">
  <li>KAUST CS 283 Deep Generative Modeling</li>
</ul>



      <br>

      
      <p><small>Powered by Jekyll and <a href="https://github.com/yaoyao-liu/minimal-light" target="_blank" rel="noopener">Minimal Light</a> theme.</small></p>
      

      </section>
      <footer>
        
      </footer>
    </div>
    <script src="/assets/js/scale.fix.js"></script>
    
  </body>
</html>
